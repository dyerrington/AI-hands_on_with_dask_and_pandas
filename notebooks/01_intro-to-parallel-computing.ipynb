{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://snipboard.io/Kx6OAi.jpg\">\n",
    "\n",
    "# Session 1. Intro to Parallel Computing\n",
    "<div style=\"margin-top: -10px; padding: 5px; line-height: 20px;\"><img src=\"https://snipboard.io/v5q47G.jpg\" style=\"width: 35px; float: left; margin-right: 10px;\"> Author:  <a href=\"http://www.linkedin.com/in/davidyerrington\">David Yerrington</a>, Data Scientist<br>San Francisco, CA</div>\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Explain the core concepts of parallel computing\n",
    "- Be familiar with types of parallel processing provided by Dask\n",
    "- Identify the major components of Dask: Collection Types and its Scheduler\n",
    "\n",
    "### Prerequisite Knowledge\n",
    "- Basic Pandas \n",
    "  - Difference between Series vs Dataframe\n",
    "  - Bitmasks, query function, selecting data\n",
    "  - Aggregations\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "We will first review some basic points to setup Python and the environment to start in [the setup guide](../environment.md).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Intro to Parallel Computing with Dask\n",
    "\n",
    "<a name=\"big-data\"></a>\n",
    "## What is \"big data\"?\n",
    "---\n",
    "\n",
    "Big data is a term used for problems that exceeds the processing capacity of typical databases.  There are still applications of data needed to understand general characteristics, that require the ability to model both predictively and structurally.  When data grows beyond the capacity of a single machine either in speed to process, size to manage, or in variety of formats, the ability to manage data requires a different solution.\n",
    "\n",
    "**The 3 V's of Bid Data**\n",
    "- **Volume**: The amount of data\n",
    "- **Variety**: The different formats of data\n",
    "- **Velocity**: The speed of which data can be analyzed\n",
    "\n",
    "> **Dave Yerrington's 4th V (unofficial big data tenet):**\n",
    "> - **Value**: It's important to assess the value of any solutions in terms of the business.  Understanding the underpinnings of cost vs benefit is even more essential in the context of big data.  It's easy to misundersatnd the 3 V's without looking at the bigger picture, connecting the value of the business cases involved.\n",
    "\n",
    "![3v](https://snipboard.io/ewbKGk.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='parallelism'></a>\n",
    "## 1.1 Parallelism\n",
    "---\n",
    "\n",
    "The conceptual basis of Big Data processing is that a data transformation can be broken down and solved through a process of computing many smaller transformations.  The simplest form of parallel processing involves independent tasks with no need to communicate with each other.\n",
    "\n",
    "- Running multiple instances to process data\n",
    "- Data can be subset and solved iteratively \n",
    "- Sub-solutions can be solved independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:  Mowing Lawns (1 Mower)\n",
    "\n",
    "A good analogy to start with involves mowing a lawn.  Think of the mower as a single function that has to complete its task over an entire lawn of grass (a dataset).  Regardless of how big the lawn is, a fixed rate mower could mow the entirety of lawn-space.\n",
    "\n",
    "![](https://snipboard.io/CR89sH.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:  Mowing Lawns (4 Mowers)\n",
    "\n",
    "With 4x the mowers, each mower technically only needs to mow 1/4 of the lawn-space to complete its task.  \n",
    "- Each mower can complete its task independently\n",
    "- Each mower operates at the same time\n",
    "\n",
    "![](https://snipboard.io/Bm3ER1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Example:  Summation\n",
    "\n",
    "It is possible to sum an entire list with a single function or basic iteration.  However, to illustrate the concept of parallel processing, breaking a list up into smaller \"chunks\" enables tasks to be computed independently.  The idea of many tasks solving portions of a larger problem is the core idea that underpins the existance of parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"../media/parallel_processing_demo_1.m4v\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"../media/parallel_processing_demo_1.m4v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:  What are some problems you've run into with Pandas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. What is Dask?\n",
    "\n",
    "![](https://snipboard.io/HNWi85.jpg)\n",
    "\n",
    "Dask is:\n",
    "\n",
    "- A Python-based parallel computing framework\n",
    "- Similiar API as Pandas\n",
    "\n",
    "### Compared to Spark / PySpark\n",
    "\n",
    "Spark is another great framework.  Spark is an all encompsing solution compared to Dask:\n",
    "\n",
    "- Also a parallel computing framework\n",
    "- Has a machine learning component like Scikit-Learn\n",
    "- Interoperable with many big data frameworks like Hadoop/HDFS\n",
    "- DataFrames / Panel-like data object\n",
    "- SQL interface\n",
    "- Graph capabilities (GraphX)\n",
    "\n",
    "Dask's area of focus is around distributed computing in Python but also includes ML now with Dask ML.  PySpark/Spark is fundamentally written in Scala (a really powerful langauge) so even if you're using Python/PySpark, all of it's bindings and functionality is relative to underlying Scala code.  Spark is an ecosystem of tools and libraries for building pipelines, streaming, graph, and ML problems. \n",
    "\n",
    "## Why Dask?\n",
    "\n",
    "As practioners of Python, it's quite nice to stay in the ecosystem of data science toolsets that are familliar.  To be able to go from exploratory analysis, to ETL, to feature engineering, to data warehouses, modeling, and production systems without having to leave Python, is a huge plus.  You can do everything in Spark, but there is a quite a lot of precedence knowledge to understand about working in an entirely different ecosystem of tools.  With Dask, you can work with all your favorite Python tools with the possibility of scaling.\n",
    "\n",
    "![](https://snipboard.io/rM7sSJ.jpg)\n",
    "\n",
    "\n",
    "> # Also:  Dask comes with Anaconda by default now!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Core Concepts\n",
    "\n",
    "We'll get into more specifics in the next notebook / session, but it's helpful to know a few basic concepts before diving into specifics.\n",
    "\n",
    "\n",
    "## 3.1 \"Dask is a graph execution engine\"\n",
    "\n",
    "> \"... specifically, a directed acyclic graph of tasks with data dependencies â€“ using ordinary Python data structures, namely dicts, tuples, functions, and arbitrary Python values\" -Dask Documentation\n",
    "\n",
    "![](https://snipboard.io/jNO1WE.jpg)\n",
    "\n",
    "> **Did You Know: DAGs**\n",
    ">\n",
    "> If you've ever used a decision tree classifier, you've used a [directed acyclic graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph).   Generally, DAGs flow in one direction only represent unidirectional ordering and model relationships between entities ([topological ordering](https://en.wikipedia.org/wiki/Topological_ordering)).  Depending on the domain of graph theory, or computer science that you think in, DAGs can describe a variety of applications.\n",
    ">\n",
    "> - Scheduling of process executions\n",
    "> - Family Trees\n",
    "> - Decision Trees\n",
    "\n",
    "When it comes to Dask, before any processing occurs, a \"plan\" is devised before any processing occurs.  The \"plan\" determines which steps and in which order a set of tasks will be performed.  \n",
    "\n",
    "## The \"plan\" is the **DAG**\n",
    "\n",
    "![](https://snipboard.io/WAE4Rv.jpg)\n",
    "\n",
    "Before anything is computed, a DAG is devised that considers the data, tasks to be ran and with available resources.  Dask processes can be used across several machines with many nodes, or on a single machine.\n",
    "\n",
    "### Question:  Why do you think the **DAG** is generated but not executed by default?\n",
    "\n",
    "## 3.2 A **Scheduler** \"runs\" the **DAG**\n",
    "\n",
    "The DAG defines the scope of work that will take place once the time comes to take action.  The \"scheduler\" takes the DAG and actually executes it.  The behavior of the scheduler is also dependent on the data and is dictated by the type of Dag data types used (more on that soon).\n",
    "\n",
    "## 4. Dask Interface Types\n",
    "\n",
    "We'll learn that you can't just take a Pandas DataFrame and plug it into Dask and expect it to work.  Generally, the API is very similar to Pandas and can be quite easy to adjust to working with the Dask API.  However, Dask is more than working with DataFrames.  Dask is fundemantally **Python** in nature and provides many object types for solving distributed problems.  We will be exploring Dask in the context of DataFrames for the majority of our sessions but it is helpful to understand the Dask ecosystem but more importantly, the interfaces that exist, and how the scheduler behaves in relation to Dask's interfaces.\n",
    "\n",
    "> ![](https://snipboard.io/N75dYH.jpg) <br>\n",
    "> Source:  [Bicortex](http://bicortex.com/data-analysis-with-dask-a-python-scale-out-parallel-computation-framework-for-big-data/)\n",
    "\n",
    "### 4.1 High Level Collections\n",
    "\n",
    "For working with datasets that are \"larger than memory\", dask provides these high level primitive types that can be processed in parellel.\n",
    "\n",
    " - Array -> `np.array`\n",
    " - Bag -> `list`\n",
    " - DataFrame -> Pandas `DataFrame`\n",
    "\n",
    "\n",
    "### 4.2 Low Level Schedulers\n",
    "\n",
    "![](https://snipboard.io/4Mwi71.jpg)\n",
    "\n",
    "Collections contain and describe data.  Schedulers execute task graphs in parallel.\n",
    "\n",
    "#### Single Machine Scheduler  \n",
    "Using a single maching, tasks are computed on a local process or thread pool.  Single machine scheduling is the default.\n",
    "\n",
    "> ##### Local Threads\n",
    "> \n",
    "> ```python\n",
    "> import dask\n",
    "> dask.config.set(scheduler='threads')  # overwrite default with threaded scheduler\n",
    "> ```\n",
    "\n",
    "> #### Local Multiprocessing\n",
    "> \n",
    "> ```python\n",
    "> import dask.multiprocessing\n",
    "> dask.config.set(scheduler='processes')  # overwrite default with multiprocessing scheduler\n",
    "> ```\n",
    "\n",
    "#### Distributed Scheduler  \n",
    "The most flexible and feature-rich scheduler.  Can run locally or on a cluster.  We could have an entire session on this topic alone.  For our sessions, we will be using the **distributed scheduler** in local mode.\n",
    "\n",
    "Some notable features with using a distributed scheduler that we will explore in the next session:\n",
    "\n",
    "- A diagnostic dashboard that can be used to measure resource utilization and task progress.\n",
    "- The docs say it can be more efficient than using the local scheduling processor with multiprocessor option.  (I found this to be true in most cases.)\n",
    "\n",
    "![](https://snipboard.io/Av73Mn.jpg)\n",
    "\n",
    "> If you've used a scheduling system like Luigi or Airflow these systems are designed with similar features.  Dask is an alternative to using the multiprocessing library or these big fancy scheduling systems but allows you to programatically configure and run paralell tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Summary\n",
    "\n",
    "## 1.  Parallel Computing \n",
    "\n",
    "- Running multiple instances to process data\n",
    "- Data can be subset and solved iteratively\n",
    "- Sub-solutions can be solved independently\n",
    "\n",
    "![](https://snipboard.io/WH6FfT.jpg)\n",
    "\n",
    "## 2.  Parallel Processing in Dask\n",
    "- Generally, processing is \"lazy\" and starts with a \"plan\"\n",
    "- The \"plan\" is the DAG\n",
    "- Each step in a DAG models tasks that will be performed\n",
    "- The DAG is not executed until requested\n",
    "\n",
    "## 3. Dask Collections and Scheduling\n",
    "- Dask provides collections that are roughly equivalent to familliar Python data types we use when working with data.\n",
    "  - `Array` -> `np.array`\n",
    "  - `Bag` -> `list`\n",
    "  - `DataFrame` -> `Pandas DataFrame`\n",
    "- The scheduler is responsible for executing the DAG (ie: the \"plan\")\n",
    "  - Local scheduling (multiprocess and threaded)\n",
    "    - Great for debugging and can work great in a pinch for speeding up slow `.apply` functions\n",
    "  - Distributed\n",
    "    - Offers the most flexibility and features.\n",
    "    - Tends to be a bit more efficient by default but this largely depends on your dataset / transformations / processsing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
